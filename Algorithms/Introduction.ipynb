{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**In a typical Reinforcement Learning (RL) problem, there is a learner and a decision maker called agent and the surrounding with which it interacts is called environment. The environment, in return, provides rewards and a new state based on the actions of the agent. So, in reinforcement learning, we do not teach an agent how it should do something but presents it with rewards whether positive or negative based on its actions. So our root question is how we formulate any problem in RL mathematically. This is where the Markov Decision Process(MDP) comes in.**\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/1.png/\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent - Environment relationship\n",
    "\n",
    "**Agent**: Software programs that make intelligent decisions and they are the learners in RL. These agents interact with the environment by actions and receive rewards based on there actions.\n",
    "\n",
    "**Environment**:It is the demonstration of the problem to be solved. Now, we can have a real-world environment or a simulated environment with which our agent will interact.\n",
    "\n",
    "**State**: This is the position of the agents at a specific time-step in the environment.So,whenever an agent performs a action the environment gives the agent reward and a new state where the agent reached by performing the action.\n",
    "\n",
    "**Anything that the agent cannot change arbitrarily is considered to be part of the environment.** In simple terms, **actions can be any decision we want the agent to learn** and **state can be anything which can be useful in choosing actions**.\n",
    "\n",
    "We do not assume that everything in the environment is unknown to the agent, for example, reward calculation is considered to be the part of the environment even though the agent knows a bit on how it’s reward is calculated as a function of its actions and states in which they are taken. This is because rewards cannot be arbitrarily changed by the agent.\n",
    "\n",
    "`Sometimes, the agent might be fully aware of its environment but still finds it difficult to maximize the reward as like we might know how to play Rubik’s cube but still cannot solve it. So, we can safely say that the agent-environment relationship represents the limit of the agent control and not it’s knowledge.`\n",
    "\n",
    "### The Markov Property\n",
    "\n",
    "**Transition**: Moving from one state to another is called Transition.\n",
    "\n",
    "**Transition Probability**: The probability that the agent will move from one state to another is called transition probability.\n",
    "\n",
    "The Markov Property state that :\n",
    "\n",
    "`\"Future is Independent of the past given the present\"`\n",
    "\n",
    "Mathematically we can express this statement as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/2.png/\">\n",
    "</p>\n",
    "\n",
    "S[t] denotes the current state of the agent and s[t+1] denotes the next state. What this equation means is that **the transition from state S[t] to S[t+1] is entirely independent of the past**. So, the RHS of the Equation means the same as LHS if the system has a Markov Property. Intuitively meaning that our current state already captures the information of the past states.\n",
    "\n",
    "### State Transition Probability :\n",
    "As we now know about transition probability we can define state Transition Probability as follows :\n",
    "\n",
    "For Markov State from S[t] to S[t+1] i.e. any other successor state , the state transition probability is given by:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/3.png/\">\n",
    "</p>\n",
    "\n",
    "We can formulate the State Transition probability into a State Transition probability matrix by :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/4.png/\" height = 300px width = 300px>\n",
    "</p>\n",
    "\n",
    "Each row in the matrix represents the probability from moving from our original or starting state to any successor state.Sum of each row is equal to 1.\n",
    "\n",
    "### Markov Process or Markov Chains:\n",
    "\n",
    "**Markov Process is the memory less random process i.e. a sequence of a random state S[1],S[2],….S[n] with a Markov Property**. So, it’s basically a sequence of states with the Markov Property.\n",
    "\n",
    "It can be defined using a set of states(S) and transition probability matrix (P).The dynamics of the environment can be fully defined using the States(S) and Transition Probability matrix(P).\n",
    "\n",
    "#### But what does a random process means ?\n",
    "\n",
    "To answer this question let’s look at a example:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/5.png/\" height = 300px width = 300px>\n",
    "</p>\n",
    "\n",
    "The edges of the tree denote transition probability. From this chain let’s take some sample. Now, suppose that we were sleeping and the according to the probability distribution there is a 0.6 chance that we will Run and 0.2 chance we sleep more and again 0.2 that we will eat ice-cream. Similarly, we can think of other sequences that we can sample from this chain.\n",
    "\n",
    "\n",
    "Some samples from the chain :\n",
    "\n",
    "- Sleep — Run — Ice-cream — Sleep\n",
    "- Sleep — Ice-cream — Ice-cream — Run\n",
    "\n",
    "In the above two sequences what we see is we get random set of States(S) (i.e. Sleep,Ice-cream,Sleep ) every time we run the chain. **Hence, Markov process is called random set of sequences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
