{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**In a typical Reinforcement Learning (RL) problem, there is a learner and a decision maker called agent and the surrounding with which it interacts is called environment. The environment, in return, provides rewards and a new state based on the actions of the agent. So, in reinforcement learning, we do not teach an agent how it should do something but presents it with rewards whether positive or negative based on its actions. So our root question is how we formulate any problem in RL mathematically. This is where the Markov Decision Process(MDP) comes in.**\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/1.png/\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent - Environment relationship\n",
    "\n",
    "**Agent**: Software programs that make intelligent decisions and they are the learners in RL. These agents interact with the environment by actions and receive rewards based on there actions.\n",
    "\n",
    "**Environment**:It is the demonstration of the problem to be solved. Now, we can have a real-world environment or a simulated environment with which our agent will interact.\n",
    "\n",
    "**State**: This is the position of the agents at a specific time-step in the environment.So,whenever an agent performs a action the environment gives the agent reward and a new state where the agent reached by performing the action.\n",
    "\n",
    "**Anything that the agent cannot change arbitrarily is considered to be part of the environment.** In simple terms, **actions can be any decision we want the agent to learn** and **state can be anything which can be useful in choosing actions**.\n",
    "\n",
    "We do not assume that everything in the environment is unknown to the agent, for example, reward calculation is considered to be the part of the environment even though the agent knows a bit on how it’s reward is calculated as a function of its actions and states in which they are taken. This is because rewards cannot be arbitrarily changed by the agent.\n",
    "\n",
    "`Sometimes, the agent might be fully aware of its environment but still finds it difficult to maximize the reward as like we might know how to play Rubik’s cube but still cannot solve it. So, we can safely say that the agent-environment relationship represents the limit of the agent control and not it’s knowledge.`\n",
    "\n",
    "### The Markov Property\n",
    "\n",
    "**Transition**: Moving from one state to another is called Transition.\n",
    "\n",
    "**Transition Probability**: The probability that the agent will move from one state to another is called transition probability.\n",
    "\n",
    "The Markov Property state that :\n",
    "\n",
    "`\"Future is Independent of the past given the present\"`\n",
    "\n",
    "Mathematically we can express this statement as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/2.png/\">\n",
    "</p>\n",
    "\n",
    "S[t] denotes the current state of the agent and s[t+1] denotes the next state. What this equation means is that **the transition from state S[t] to S[t+1] is entirely independent of the past**. So, the RHS of the Equation means the same as LHS if the system has a Markov Property. Intuitively meaning that our current state already captures the information of the past states.\n",
    "\n",
    "### State Transition Probability :\n",
    "As we now know about transition probability we can define state Transition Probability as follows :\n",
    "\n",
    "For Markov State from S[t] to S[t+1] i.e. any other successor state , the state transition probability is given by:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/3.png/\">\n",
    "</p>\n",
    "\n",
    "We can formulate the State Transition probability into a State Transition probability matrix by :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/4.png/\" height = 300px width = 300px>\n",
    "</p>\n",
    "\n",
    "Each row in the matrix represents the probability from moving from our original or starting state to any successor state.Sum of each row is equal to 1.\n",
    "\n",
    "### Markov Process or Markov Chains:\n",
    "\n",
    "**Markov Process is the memory less random process i.e. a sequence of a random state S[1],S[2],….S[n] with a Markov Property**. So, it’s basically a sequence of states with the Markov Property.\n",
    "\n",
    "It can be defined using a set of states(S) and transition probability matrix (P).The dynamics of the environment can be fully defined using the States(S) and Transition Probability matrix(P).\n",
    "\n",
    "#### But what does a random process means ?\n",
    "\n",
    "To answer this question let’s look at a example:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/5.png/\" height = 300px width = 300px>\n",
    "</p>\n",
    "\n",
    "The edges of the tree denote transition probability. From this chain let’s take some sample. Now, suppose that we were sleeping and the according to the probability distribution there is a 0.6 chance that we will Run and 0.2 chance we sleep more and again 0.2 that we will eat ice-cream. Similarly, we can think of other sequences that we can sample from this chain.\n",
    "\n",
    "\n",
    "Some samples from the chain :\n",
    "\n",
    "- Sleep — Run — Ice-cream — Sleep\n",
    "- Sleep — Ice-cream — Ice-cream — Run\n",
    "\n",
    "In the above two sequences what we see is we get random set of States(S) (i.e. Sleep,Ice-cream,Sleep ) every time we run the chain. **Hence, Markov process is called random set of sequences.**\n",
    "\n",
    "### Reward and Returns\n",
    "\n",
    "**Rewards are the numerical values that the agent receives on performing some action at some state(s) in the environment. The numerical value can be positive or negative based on the actions of the agent.**\n",
    "\n",
    "In Reinforcement learning, we care about maximizing the cumulative reward (all the rewards agent receives from the environment) instead of, the reward agent receives from the current state(also called immediate reward). This \n",
    "total sum of reward the agent receives from the environment is called returns.\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/6.png/\">\n",
    "</p>\n",
    "\n",
    "r[t+1] is the reward received by the agent at time step t[0] while performing an action(a) to move from one state to another. Similarly, r[t+2] is the reward received by the agent at time step t[1] by performing an action to move to another state. And, r[T] is the reward received by the agent by at the final time step by performing an action to move to another state.\n",
    "\n",
    "### Episodic and Continuous Tasks\n",
    "\n",
    "**Episodic Tasks**: These are the tasks that have a terminal state (end state).We can say they have finite states. For example, in racing games, we start the game (start the race) and play it until the game is over (race ends!). This is called an episode. Once we restart the game it will start from an initial state and hence, every episode is independent.\n",
    "\n",
    "**Continuous Tasks**: These are the tasks that have no ends i.e. they don’t have any terminal state.These types of tasks will never end.For example, Learning how to code!\n",
    "\n",
    "Now, **it’s easy to calculate the returns from the episodic tasks as they will eventually end but what about continuous tasks, as it will go on and on forever. The returns from sum up to infinity! So, how we define returns for continuous tasks?**\n",
    "\n",
    "This is where we need **Discount factor(ɤ)**.\n",
    "\n",
    "**Discount Factor (ɤ)**: It determines how much importance is to be given to the immediate reward and future rewards. This basically helps us to avoid infinity as a reward in continuous tasks. It has a value between 0 and 1. A value of 0 means that more importance is given to the immediate reward and a value of 1 means that more importance is given to future rewards.\n",
    "\n",
    "**In practice, a discount factor of 0 will never learn as it only considers immediate reward and a discount factor of 1 will go on for future rewards which may lead to infinity. Therefore, the optimal value for the discount factor lies between 0.2 to 0.8.**\n",
    "\n",
    "So, we can define returns using discount factor as follows:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/7.png/\">\n",
    "</p>\n",
    "\n",
    "Let’s understand it with an example,\n",
    "\n",
    "suppose you live at a place where you face water scarcity so if someone comes to you and say that he will give you 100 liters of water!(assume please!) for the next 15 hours as a function of some parameter (ɤ).Let’s look at two possibilities : \n",
    "\n",
    "One with discount factor (ɤ) 0.8 :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/8.png/\">\n",
    "</p>\n",
    "\n",
    "This means that we should wait till 15th hour because the decrease is not very significant , so it’s still worth to go till the end.This means that we are also interested in future rewards.So, if the discount factor is close to 1 then we will make a effort to go to end as the reward are of significant importance.\n",
    "\n",
    "Second, with discount factor (ɤ) 0.2 :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/9.png/\">\n",
    "</p>\n",
    "\n",
    "This means that we are more interested in early rewards as the rewards are getting significantly low at hour.So, we might not want to wait till the end (till 15th hour) as it will be worthless.So, if the discount factor is close to zero then immediate rewards are more important that the future.\n",
    "\n",
    "#### So which value of discount factor to use ?\n",
    "\n",
    "It depends on the task that we want to train an agent for. Suppose, in a chess game, the goal is to defeat the opponent’s king. If we give importance to the immediate rewards like a reward on pawn defeat any opponent player then the agent will learn to perform these sub-goals no matter if his players are also defeated. So, in this task future rewards are more important. In some, we might prefer to use immediate rewards like the water example we saw earlier.\n",
    "\n",
    "### Markov Reward Process:\n",
    "\n",
    "**As the name suggests, MDPs are the Markov chains with values judgement.Basically, we get a value from every state our agent is in.**\n",
    "\n",
    "Mathematically, we define Markov Reward Process as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/10.png/\">\n",
    "</p>\n",
    "\n",
    "What this equation means is how much reward (Rs) we get from a particular state S[t]. This tells us the immediate reward from that particular state our agent is in. As we will see in the next story how we maximize these rewards from each state our agent is in. In simple terms, maximizing the cumulative reward we get from each state.\n",
    "\n",
    "We define MRP as (S,P,R,ɤ) ,where :\n",
    "\n",
    "- S is a set of states,\n",
    "- P is the Transition Probability Matrix,\n",
    "- R is the Reward function , we saw earlier,\n",
    "- ɤ is the discount factor\n",
    "\n",
    "### Policy Function and Value Function\n",
    "\n",
    "**Value Function determines how good it is for the agent to be in a particular state.** Of course, to determine how good it will be to be in a particular state it must depend on some actions that it will take. This is where policy comes in. **A policy defines what actions to perform in a particular state s.**\n",
    "\n",
    "A policy is a simple function, that defines a probability distribution over Actions (a∈ A) for each state (s ∈ S). If an agent at time t follows a policy π then π(a|s) is the probability that agent with taking action (a ) at particular time step (s).In Reinforcement Learning the experience of the agent determines the change in policy. \n",
    "\n",
    "Mathematically, a policy is defined as follows :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/11.png/\">\n",
    "</p>\n",
    "\n",
    "Now, how we find a value of a state.The value of state s, when agent is following a policy π which is denoted by vπ(s) is the expected return starting from s and following a policy π for the next states,until we reach the terminal state.We can formulate this as :(This function is also called State-value Function):\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/12.png/\">\n",
    "</p>\n",
    "\n",
    "This equation gives us the expected returns starting from state(s) and going to successor states thereafter, with the policy π. One thing to note is the returns we get is stochastic whereas the value of a state is not stochastic.\n",
    "\n",
    "It is the expectation of returns from start state s and thereafter, to any other state. And also note that the value of the terminal state (if there is any) is zero. Let’s look at an example :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/13.png/\" height = 500px width = 500px>\n",
    "</p>\n",
    "\n",
    "Suppose our start state is Class 2, and we move to Class 3 then Pass then Sleep.In short, Class 2 > Class 3 > Pass > Sleep.\n",
    "\n",
    "Our expected return is with discount factor 0.5:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/14.jpg/\">\n",
    "</p>\n",
    "\n",
    "### Bellman Equation for Value Function:\n",
    "\n",
    "Bellman Equation helps us to find optimal policies and value function. We know that our policy changes with experience so we will have different value function according to different policies. Optimal value function is one which gives maximum value compared to all other value functions.\n",
    "\n",
    "Bellman Equation states that value function can be decomposed into two parts:\n",
    "\n",
    "- Immediate Reward, R[t+1]\n",
    "- Discounted value of successor states,\n",
    "\n",
    "Mathematically, we can define Bellman Equation as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/14.png/\">\n",
    "</p>\n",
    "\n",
    "Let’s understand what this equation says with a help of an example :\n",
    "\n",
    "\n",
    "\n",
    "Suppose, there is a robot in some state (s) and then he moves from this state to some other state (s’). Now, the question is how good it was for the robot to be in the state(s). Using the Bellman equation, we can that it is the expectation of reward it got on leaving the state(s) plus the value of the state (s’) he moved to.\n",
    "\n",
    "Let’s look at another example :\n",
    "\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/15.png/\">\n",
    "</p>\n",
    "\n",
    "\n",
    "We want to know the value of state s.The value of state(s) is the reward we got upon leaving that state, plus the discounted value of the state we landed upon multiplied by the transition probability that we will move into it.\n",
    "\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/16.png/\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "The above equation can be expressed in matrix form as follows :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/17.png/\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Where v is the value of state we were in, which is equal to the immediate reward plus the discounted value of the next state multiplied by the probability of moving into that state.\n",
    "\n",
    "**The running time complexity for this computation is O(n³). Therefore, this is clearly not a practical solution for solving larger MRPs (same for MDPs, as well).**\n",
    "\n",
    "### Markov Decision Process:\n",
    "\n",
    "It is Markov Reward Process with a decisions.Everything is same like MRP but now we have actual agency that makes decisions or take actions.\n",
    "\n",
    "It a tuple of (S, A, P, R, 𝛾) where:\n",
    "\n",
    "- S is a set of states,\n",
    "- A is the set of actions agent can choose to take,\n",
    "- P is the transition Probability Matrix,\n",
    "- R is the Reward accumulated by the actions of the agent,\n",
    "- 𝛾 is the discount factor.\n",
    "\n",
    "P and R will have slight change w.r.t actions as follows :\n",
    "\n",
    "\n",
    "\n",
    "#### Transition Probability Matrix:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/18.png/\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "#### Reward Function:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/19.png/\">\n",
    "</p>\n",
    "\n",
    "Now, our reward function is dependent on the action.\n",
    "\n",
    "Till now we have talked about getting a reward (r) when our agent goes through a set of states (s) following a policy π.Actually,in Markov Decision Process(MDP) the policy is the mechanism to take decisions .So now we have a mechanism which will choose to take an action.\n",
    "\n",
    "Policies in an MDP depends on the current state.They do not depend on the history.That’s the Markov Property.So, the current state we are in characterizes the history.\n",
    "\n",
    "**We have already seen how good it is for the agent to be in a particular state(State-value function).Now, let’s see how good it is to take a particular action following a policy π from state s (Action-Value Function).**\n",
    "\n",
    "\n",
    "### State-action value function or Q-Function:\n",
    "\n",
    "This function specifies the how good it is for the agent to take action (a) in a state (s) with a policy π.\n",
    "\n",
    "Mathematically, we can define State-action value function as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/20.png/\">\n",
    "</p>\n",
    "\n",
    "Basically, it tells us the value of performing a certain action(a) in a state(s) with a policy π.\n",
    "\n",
    "Let’s look at a example of Markov Decision Process :\n",
    "<p>\n",
    "        <img src = \"assets/21.png/\">\n",
    "</p>\n",
    "\n",
    "Now, we can see that there are no more probabilities.In fact now our agent has choices to make like after waking up ,we can choose to watch netflix or code and debug.Of course the actions of the agent are defined w.r.t some policy π and will be get the reward accordingly.\n",
    "\n",
    "### Bellman Expectation Equation:\n",
    "\n",
    "The Bellman equation is given as:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/14.png/\">\n",
    "</p>\n",
    "\n",
    "From the above equation, we can see that the value of a state can be decomposed into immediate reward(R[t+1]) plus the value of successor state(v[S (t+1)]) with a discount factor( 𝛾). This still stands for Bellman Expectation Equation. But now what we are doing is we are finding the value of a particular state subjected to some policy(π).\n",
    "\n",
    "This is the difference between the Bellman Equation and the Bellman Expectation Equation.\n",
    "\n",
    "Mathematically we can define Bellman Expectation Equation as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/22.png/\">\n",
    "</p>\n",
    "\n",
    "The above equation tells us that the value of a particular state is determined by the immediate reward plus the value of successor states when we are following a certain policy(π).\n",
    "\n",
    "\n",
    "Similarly, we can express our state-action Value function (Q-Function) as follows :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/23.png/\">\n",
    "</p>\n",
    "\n",
    "\n",
    "From the above equation, we can see that the State-Action Value of a state can be decomposed into the immediate reward we get on performing a certain action in state(s) and moving to another state(s’) plus the discounted value of the state-action value of the state(s’) with respect to the some action(a) our agent will take from that state on-wards.\n",
    "\n",
    "First, let’s understand Bellman Expectation Equation for State-Value Function with the help of a backup diagram:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/24.png/\">\n",
    "</p>\n",
    "\n",
    "This backup diagram describes the value of being in a particular state. From the state s there is some probability that we take both the actions. There is a Q-value(State-action value function) for each of the action. We average the Q-values which tells us how good it is to be in a particular state. Basically, it defines Vπ(s).\n",
    "\n",
    "Mathematically, we can define it as follows:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/25.png/\">\n",
    "</p>\n",
    "\n",
    "\n",
    "This equation also tells us the connection between State-Value function and State-Action Value Function.\n",
    "\n",
    "Now, let’s look at the backup diagram for State-Action Value Function:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/26.png/\">\n",
    "</p>\n",
    "\n",
    "\n",
    "This backup diagram says that suppose we start off by taking some action(a). So, because of the action(a) the agent might be blown to any of these states by the environment. Therefore, we are asking the question, how good it is to take action(a)?\n",
    "\n",
    "We again average the state-values of both the states, added with an immediate reward which tells us how good it is to take a particular action(a).This defines our qπ(s,a).\n",
    "\n",
    "Mathematically, we can define this as follows :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/27.png/\">\n",
    "</p>\n",
    "where P is the Transition Probability.\n",
    "\n",
    "\n",
    "Now let’s stitch these backup diagrams together to define State-Value Function, Vπ(s):\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/28.png/\">\n",
    "</p>\n",
    "\n",
    "From the above diagram, if our agent is in some state(s) and from that state suppose our agent can take two actions due to which environment might take our agent to any of the states(s’). Note that the probability of the action our agent might take from state s is weighted by our policy and after taking that action the probability that we land in any of the states(s’) is weighted by the environment.\n",
    "\n",
    "Now our question is, how good it is to be in state(s) after taking some action and landing on another state(s’) and following our policy(π) after that?\n",
    "\n",
    "It is similar to what we have done before,we are going to average the value of successor states(s’) with some transition probability(P) weighted with our policy.\n",
    "\n",
    "Mathematically, we can define it as follows:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/29.png/\">\n",
    "</p>\n",
    "\n",
    "Now, let’s do the same for State-Action Value Function, qπ(s,a) :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/30.png/\">\n",
    "</p>\n",
    "\n",
    "It’s very similar to what we did in State-Value Function and just it’s inverse, so this diagram basically says that our agent take some action(a) because of which the environment might land us on any of the states(s), then from that state we can choose to take any actions(a’) weighted with the probability of our policy(π). Again, we average them together and that gives us how good it is to take a particular action following a particular policy(π) all along.\n",
    "\n",
    "\n",
    "Mathematically, this can be expressed as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/31.png/\">\n",
    "</p>\n",
    "\n",
    "So, **this is how we can formulate Bellman Expectation Equation for a given MDP to find it’s State-Value Function and State-Action Value Function. But, it does not tell us the best way to behave in an MDP. For that let’s see what is meant by Optimal Value and Optimal Policy Function.**\n",
    "\n",
    "### Optimal Value Function:\n",
    "\n",
    "In an MDP environment, there are many different value functions according to different policies. The optimal Value function is one which yields maximum value compared to all other value function. When we say we are solving an MDP it actually means we are finding the Optimal Value Function.\n",
    "\n",
    "So, mathematically Optimal State-Value Function can be expressed as :\n",
    "\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/32.png/\">\n",
    "</p>\n",
    "\n",
    "In the above formula, v∗(s) tells us what is the maximum reward we can get from the system.\n",
    "\n",
    "Similarly, Optimal State-Action Value Function tells us the maximum reward we are going to get if we are in state s and taking action a from there on-wards.\n",
    "\n",
    "\n",
    "Mathematically, It can be defined as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/33.png/\">\n",
    "</p>\n",
    "\n",
    "#### Optimal State-Value Function :It is the maximum Value function over all policies.\n",
    "#### Optimal State-Action Value Function: It is the maximum action-value function over all policies.\n",
    "\n",
    "**We say that one policy(π) is better than other policy (π’) if the value function with the policy π for all states is greater than the value function with the policy π’ for all states. Intuitively, it can be expressed as :**\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/34.png/\">\n",
    "</p>\n",
    "\n",
    "#### Optimal Policy is one which results in optimal value function.\n",
    "\n",
    "Note that, **there can be more than one optimal policy in a MDP. But, all optimal policy achieve the same optimal value function and optimal state-action Value Function(Q-function).**\n",
    "\n",
    "\n",
    "Now, the question arises how we find Optimal Policy.\n",
    "\n",
    "We find an optimal policy by maximizing over q*(s, a) i.e. our optimal state-action value function.We solve q*(s,a) and then we pick the action that gives us most optimal state-action value function(q*(s,a)).\n",
    "\n",
    "The above statement can be expressed as:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/35.png/\">\n",
    "</p>\n",
    "\n",
    "**What this says is that for a state s we pick the action a with probability 1, if it gives us the maximum q*(s,a). So, if we know q*(s,a) we can get an optimal policy from it.**\n",
    "\n",
    "Let’s understand it with an example :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/36.png/\">\n",
    "</p>\n",
    "\n",
    "In this example, the red arcs are the optimal policy which means that if our agent follows this path it will yield maximum reward from this MDP. Also, by seeing the q* values for each state we can say the actions our agent will take that yields maximum reward. So, optimal policy always takes action with higher q* value(State-Action Value Function). For example, in the state with value 8, there is q* with value 0 and 8. Our agent chooses the one with greater q* value i.e. 8.\n",
    "\n",
    "**Now, the question arises, How do we find these q*(s,a) values ?**\n",
    "\n",
    "This is where Bellman Optimality Equation comes into play.\n",
    "\n",
    "### Bellman Optimality Equation:\n",
    "\n",
    "**The Optimal Value Function is recursively related to the Bellman Optimality Equation.**\n",
    "\n",
    "Bellman Optimality equation is the same as Bellman Expectation Equation but the only difference is instead of taking the average of the actions our agent can take we take the action with the max value.\n",
    "\n",
    "Let’s understand this with the help of Backup diagram:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/37.png/\">\n",
    "</p>\n",
    "\n",
    "Suppose our agent is in state S and from that state it can take two actions (a). So, we look at the action-values for each of the actions and unlike, Bellman Expectation Equation, instead of taking the average our agent takes the action with greater q* value. This gives us the value of being in the state S.\n",
    "\n",
    "Mathematically, this can be expressed as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/38.png/\">\n",
    "</p>\n",
    "\n",
    "Similarly, let’s define Bellman Optimality Equation for State-Action Value Function (Q-Function).\n",
    "\n",
    "Let’s look at the Backup Diagram for State-Action Value Function(Q-Function):\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/39.png/\">\n",
    "</p>\n",
    "\n",
    "Suppose, our agent has taken an action a in some state s. Now, it’s on the environment that it might blow us to any of these states (s’). We still take the average of the values of both the states, but the only difference is in Bellman Optimality Equation we know the optimal values of each of the states.Unlike in Bellman Expectation Equation we just knew the value of the states.\n",
    "\n",
    "\n",
    "Mathematically, this can be expressed as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/40.png/\">\n",
    "</p>\n",
    "\n",
    "Let’s again stitch these backup diagrams for State-Value Function :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/41.png/\">\n",
    "</p>\n",
    "\n",
    "Suppose our agent is in state s and from that state it took some action (a) where the probability of taking that action is weighted by the policy. And because of the action (a), the agent might get blown to any of the states(s’) where probability is weighted by the environment. In order to find the value of state S we simply average the Optimal values of the States(s’). This gives us the value of being in state S.\n",
    "\n",
    "Mathematically, this can be expressed as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/42.png/\">\n",
    "</p>\n",
    "\n",
    "The max in the equation is because we are maximizing the actions the agent can take in the upper arcs. This equation also shows how we can relate V* function to itself.\n",
    "\n",
    "Now, let’s look at the Bellman Optimality Equation for State-Action Value Function,q*(s,a) :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/43.png/\">\n",
    "</p>\n",
    "\n",
    "Suppose, our agent was in state s and it took some action(a). Because of that action, the environment might land our agent to any of the states (s’) and from these states we get to maximize the action our agent will take i.e. choosing the action with maximum q* value. We back that up to the top and that tells us the value of the action a.\n",
    "\n",
    "Mathematically, this can be expressed as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/44.png/\">\n",
    "</p>\n",
    "\n",
    "Let’s look at an example to understand it better :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/45.png/\">\n",
    "</p>\n",
    "\n",
    "Look at the red arrows, suppose we wish to find the value of state with value 6 (in red), as we can see we get a reward of -1 if our agent chooses Facebook and a reward of -2 if our agent choose to study. In order to find the value of state in red, we will use the Bellman Optimality Equation for State-Value Function i.e. considering the other two states have optimal value we are going to take an average and maximize for both the action (choose the one that gives maximum value). So, from the diagram we can see that going to Facebook yields a value of 5 for our red state and going to study yields a value of 6 and then we maximize over the two which gives us 6 as the answer.\n",
    "\n",
    "\n",
    "Now, **how do we solve Bellman Optimality Equation for large MDPs.** In order to do so we use Dynamic Programming algorithms like Policy iteration and Value iteration and other methods like Q-Learning and SARSA that are used for Temporal Difference Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
