{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**In a typical Reinforcement Learning (RL) problem, there is a learner and a decision maker called agent and the surrounding with which it interacts is called environment. The environment, in return, provides rewards and a new state based on the actions of the agent. So, in reinforcement learning, we do not teach an agent how it should do something but presents it with rewards whether positive or negative based on its actions. So our root question is how we formulate any problem in RL mathematically. This is where the Markov Decision Process(MDP) comes in.**\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/1.png/\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent - Environment relationship\n",
    "\n",
    "**Agent**: Software programs that make intelligent decisions and they are the learners in RL. These agents interact with the environment by actions and receive rewards based on there actions.\n",
    "\n",
    "**Environment**:It is the demonstration of the problem to be solved. Now, we can have a real-world environment or a simulated environment with which our agent will interact.\n",
    "\n",
    "**State**: This is the position of the agents at a specific time-step in the environment.So,whenever an agent performs a action the environment gives the agent reward and a new state where the agent reached by performing the action.\n",
    "\n",
    "**Anything that the agent cannot change arbitrarily is considered to be part of the environment.** In simple terms, **actions can be any decision we want the agent to learn** and **state can be anything which can be useful in choosing actions**.\n",
    "\n",
    "We do not assume that everything in the environment is unknown to the agent, for example, reward calculation is considered to be the part of the environment even though the agent knows a bit on how it’s reward is calculated as a function of its actions and states in which they are taken. This is because rewards cannot be arbitrarily changed by the agent.\n",
    "\n",
    "`Sometimes, the agent might be fully aware of its environment but still finds it difficult to maximize the reward as like we might know how to play Rubik’s cube but still cannot solve it. So, we can safely say that the agent-environment relationship represents the limit of the agent control and not it’s knowledge.`\n",
    "\n",
    "### The Markov Property\n",
    "\n",
    "**Transition**: Moving from one state to another is called Transition.\n",
    "\n",
    "**Transition Probability**: The probability that the agent will move from one state to another is called transition probability.\n",
    "\n",
    "The Markov Property state that :\n",
    "\n",
    "`\"Future is Independent of the past given the present\"`\n",
    "\n",
    "Mathematically we can express this statement as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/2.png/\">\n",
    "</p>\n",
    "\n",
    "S[t] denotes the current state of the agent and s[t+1] denotes the next state. What this equation means is that **the transition from state S[t] to S[t+1] is entirely independent of the past**. So, the RHS of the Equation means the same as LHS if the system has a Markov Property. Intuitively meaning that our current state already captures the information of the past states.\n",
    "\n",
    "### State Transition Probability :\n",
    "As we now know about transition probability we can define state Transition Probability as follows :\n",
    "\n",
    "For Markov State from S[t] to S[t+1] i.e. any other successor state , the state transition probability is given by:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/3.png/\">\n",
    "</p>\n",
    "\n",
    "We can formulate the State Transition probability into a State Transition probability matrix by :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/4.png/\" height = 300px width = 300px>\n",
    "</p>\n",
    "\n",
    "Each row in the matrix represents the probability from moving from our original or starting state to any successor state.Sum of each row is equal to 1.\n",
    "\n",
    "### Markov Process or Markov Chains:\n",
    "\n",
    "**Markov Process is the memory less random process i.e. a sequence of a random state S[1],S[2],….S[n] with a Markov Property**. So, it’s basically a sequence of states with the Markov Property.\n",
    "\n",
    "It can be defined using a set of states(S) and transition probability matrix (P).The dynamics of the environment can be fully defined using the States(S) and Transition Probability matrix(P).\n",
    "\n",
    "#### But what does a random process means ?\n",
    "\n",
    "To answer this question let’s look at a example:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/5.png/\" height = 300px width = 300px>\n",
    "</p>\n",
    "\n",
    "The edges of the tree denote transition probability. From this chain let’s take some sample. Now, suppose that we were sleeping and the according to the probability distribution there is a 0.6 chance that we will Run and 0.2 chance we sleep more and again 0.2 that we will eat ice-cream. Similarly, we can think of other sequences that we can sample from this chain.\n",
    "\n",
    "\n",
    "Some samples from the chain :\n",
    "\n",
    "- Sleep — Run — Ice-cream — Sleep\n",
    "- Sleep — Ice-cream — Ice-cream — Run\n",
    "\n",
    "In the above two sequences what we see is we get random set of States(S) (i.e. Sleep,Ice-cream,Sleep ) every time we run the chain. **Hence, Markov process is called random set of sequences.**\n",
    "\n",
    "### Reward and Returns\n",
    "\n",
    "**Rewards are the numerical values that the agent receives on performing some action at some state(s) in the environment. The numerical value can be positive or negative based on the actions of the agent.**\n",
    "\n",
    "In Reinforcement learning, we care about maximizing the cumulative reward (all the rewards agent receives from the environment) instead of, the reward agent receives from the current state(also called immediate reward). This \n",
    "total sum of reward the agent receives from the environment is called returns.\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/6.png/\">\n",
    "</p>\n",
    "\n",
    "r[t+1] is the reward received by the agent at time step t[0] while performing an action(a) to move from one state to another. Similarly, r[t+2] is the reward received by the agent at time step t[1] by performing an action to move to another state. And, r[T] is the reward received by the agent by at the final time step by performing an action to move to another state.\n",
    "\n",
    "### Episodic and Continuous Tasks\n",
    "\n",
    "**Episodic Tasks**: These are the tasks that have a terminal state (end state).We can say they have finite states. For example, in racing games, we start the game (start the race) and play it until the game is over (race ends!). This is called an episode. Once we restart the game it will start from an initial state and hence, every episode is independent.\n",
    "\n",
    "**Continuous Tasks**: These are the tasks that have no ends i.e. they don’t have any terminal state.These types of tasks will never end.For example, Learning how to code!\n",
    "\n",
    "Now, **it’s easy to calculate the returns from the episodic tasks as they will eventually end but what about continuous tasks, as it will go on and on forever. The returns from sum up to infinity! So, how we define returns for continuous tasks?**\n",
    "\n",
    "This is where we need **Discount factor(ɤ)**.\n",
    "\n",
    "**Discount Factor (ɤ)**: It determines how much importance is to be given to the immediate reward and future rewards. This basically helps us to avoid infinity as a reward in continuous tasks. It has a value between 0 and 1. A value of 0 means that more importance is given to the immediate reward and a value of 1 means that more importance is given to future rewards.\n",
    "\n",
    "**In practice, a discount factor of 0 will never learn as it only considers immediate reward and a discount factor of 1 will go on for future rewards which may lead to infinity. Therefore, the optimal value for the discount factor lies between 0.2 to 0.8.**\n",
    "\n",
    "So, we can define returns using discount factor as follows:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/7.png/\">\n",
    "</p>\n",
    "\n",
    "Let’s understand it with an example,\n",
    "\n",
    "suppose you live at a place where you face water scarcity so if someone comes to you and say that he will give you 100 liters of water!(assume please!) for the next 15 hours as a function of some parameter (ɤ).Let’s look at two possibilities : \n",
    "\n",
    "One with discount factor (ɤ) 0.8 :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/8.png/\">\n",
    "</p>\n",
    "\n",
    "This means that we should wait till 15th hour because the decrease is not very significant , so it’s still worth to go till the end.This means that we are also interested in future rewards.So, if the discount factor is close to 1 then we will make a effort to go to end as the reward are of significant importance.\n",
    "\n",
    "Second, with discount factor (ɤ) 0.2 :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/9.png/\">\n",
    "</p>\n",
    "\n",
    "This means that we are more interested in early rewards as the rewards are getting significantly low at hour.So, we might not want to wait till the end (till 15th hour) as it will be worthless.So, if the discount factor is close to zero then immediate rewards are more important that the future.\n",
    "\n",
    "#### So which value of discount factor to use ?\n",
    "\n",
    "It depends on the task that we want to train an agent for. Suppose, in a chess game, the goal is to defeat the opponent’s king. If we give importance to the immediate rewards like a reward on pawn defeat any opponent player then the agent will learn to perform these sub-goals no matter if his players are also defeated. So, in this task future rewards are more important. In some, we might prefer to use immediate rewards like the water example we saw earlier.\n",
    "\n",
    "### Markov Reward Process:\n",
    "\n",
    "**As the name suggests, MDPs are the Markov chains with values judgement.Basically, we get a value from every state our agent is in.**\n",
    "\n",
    "Mathematically, we define Markov Reward Process as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/10.png/\">\n",
    "</p>\n",
    "\n",
    "What this equation means is how much reward (Rs) we get from a particular state S[t]. This tells us the immediate reward from that particular state our agent is in. As we will see in the next story how we maximize these rewards from each state our agent is in. In simple terms, maximizing the cumulative reward we get from each state.\n",
    "\n",
    "We define MRP as (S,P,R,ɤ) ,where :\n",
    "\n",
    "- S is a set of states,\n",
    "- P is the Transition Probability Matrix,\n",
    "- R is the Reward function , we saw earlier,\n",
    "- ɤ is the discount factor\n",
    "\n",
    "### Policy Function and Value Function\n",
    "\n",
    "**Value Function determines how good it is for the agent to be in a particular state.** Of course, to determine how good it will be to be in a particular state it must depend on some actions that it will take. This is where policy comes in. **A policy defines what actions to perform in a particular state s.**\n",
    "\n",
    "A policy is a simple function, that defines a probability distribution over Actions (a∈ A) for each state (s ∈ S). If an agent at time t follows a policy π then π(a|s) is the probability that agent with taking action (a ) at particular time step (s).In Reinforcement Learning the experience of the agent determines the change in policy. \n",
    "\n",
    "Mathematically, a policy is defined as follows :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/11.png/\">\n",
    "</p>\n",
    "\n",
    "Now, how we find a value of a state.The value of state s, when agent is following a policy π which is denoted by vπ(s) is the expected return starting from s and following a policy π for the next states,until we reach the terminal state.We can formulate this as :(This function is also called State-value Function):\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/12.png/\">\n",
    "</p>\n",
    "\n",
    "This equation gives us the expected returns starting from state(s) and going to successor states thereafter, with the policy π. One thing to note is the returns we get is stochastic whereas the value of a state is not stochastic.\n",
    "\n",
    "It is the expectation of returns from start state s and thereafter, to any other state. And also note that the value of the terminal state (if there is any) is zero. Let’s look at an example :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/13.png/\" height = 500px width = 500px>\n",
    "</p>\n",
    "\n",
    "Suppose our start state is Class 2, and we move to Class 3 then Pass then Sleep.In short, Class 2 > Class 3 > Pass > Sleep.\n",
    "\n",
    "Our expected return is with discount factor 0.5:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/14.jpg/\">\n",
    "</p>\n",
    "\n",
    "### Bellman Equation for Value Function:\n",
    "\n",
    "Bellman Equation helps us to find optimal policies and value function. We know that our policy changes with experience so we will have different value function according to different policies. Optimal value function is one which gives maximum value compared to all other value functions.\n",
    "\n",
    "Bellman Equation states that value function can be decomposed into two parts:\n",
    "\n",
    "- Immediate Reward, R[t+1]\n",
    "- Discounted value of successor states,\n",
    "\n",
    "Mathematically, we can define Bellman Equation as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/14.png/\">\n",
    "</p>\n",
    "\n",
    "Let’s understand what this equation says with a help of an example :\n",
    "\n",
    "\n",
    "\n",
    "Suppose, there is a robot in some state (s) and then he moves from this state to some other state (s’). Now, the question is how good it was for the robot to be in the state(s). Using the Bellman equation, we can that it is the expectation of reward it got on leaving the state(s) plus the value of the state (s’) he moved to.\n",
    "\n",
    "Let’s look at another example :\n",
    "\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/15.png/\">\n",
    "</p>\n",
    "\n",
    "\n",
    "We want to know the value of state s.The value of state(s) is the reward we got upon leaving that state, plus the discounted value of the state we landed upon multiplied by the transition probability that we will move into it.\n",
    "\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/16.png/\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "The above equation can be expressed in matrix form as follows :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/17.png/\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Where v is the value of state we were in, which is equal to the immediate reward plus the discounted value of the next state multiplied by the probability of moving into that state.\n",
    "\n",
    "**The running time complexity for this computation is O(n³). Therefore, this is clearly not a practical solution for solving larger MRPs (same for MDPs, as well).**\n",
    "\n",
    "### Markov Decision Process:\n",
    "\n",
    "It is Markov Reward Process with a decisions.Everything is same like MRP but now we have actual agency that makes decisions or take actions.\n",
    "\n",
    "It a tuple of (S, A, P, R, 𝛾) where:\n",
    "\n",
    "- S is a set of states,\n",
    "- A is the set of actions agent can choose to take,\n",
    "- P is the transition Probability Matrix,\n",
    "- R is the Reward accumulated by the actions of the agent,\n",
    "- 𝛾 is the discount factor.\n",
    "\n",
    "P and R will have slight change w.r.t actions as follows :\n",
    "\n",
    "\n",
    "\n",
    "#### Transition Probability Matrix:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/18.png/\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "#### Reward Function:\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/19.png/\">\n",
    "</p>\n",
    "\n",
    "Now, our reward function is dependent on the action.\n",
    "\n",
    "Till now we have talked about getting a reward (r) when our agent goes through a set of states (s) following a policy π.Actually,in Markov Decision Process(MDP) the policy is the mechanism to take decisions .So now we have a mechanism which will choose to take an action.\n",
    "\n",
    "Policies in an MDP depends on the current state.They do not depend on the history.That’s the Markov Property.So, the current state we are in characterizes the history.\n",
    "\n",
    "**We have already seen how good it is for the agent to be in a particular state(State-value function).Now, let’s see how good it is to take a particular action following a policy π from state s (Action-Value Function).**\n",
    "\n",
    "\n",
    "### State-action value function or Q-Function:\n",
    "\n",
    "This function specifies the how good it is for the agent to take action (a) in a state (s) with a policy π.\n",
    "\n",
    "Mathematically, we can define State-action value function as :\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/20.png/\">\n",
    "</p>\n",
    "\n",
    "Basically, it tells us the value of performing a certain action(a) in a state(s) with a policy π.\n",
    "\n",
    "Let’s look at a example of Markov Decision Process :\n",
    "<p>\n",
    "        <img src = \"assets/21.png/\">\n",
    "</p>\n",
    "\n",
    "Now, we can see that there are no more probabilities.In fact now our agent has choices to make like after waking up ,we can choose to watch netflix or code and debug.Of course the actions of the agent are defined w.r.t some policy π and will be get the reward accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
